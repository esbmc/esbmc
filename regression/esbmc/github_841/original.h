// This file is computer-generated by onnx2c 
// (TODO: add creating command line here)
// (TODO: print creation date here )

// ONNX model:
// produced by pytorch, version 1.11.0
// ONNX IR version: 9
// Model documentation: 
/*

*/

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdint.h>
#include <string.h>
#define MAX(X,Y) ( X > Y ? X : Y)
#define MIN(X,Y) ( X < Y ? X : Y)
#define CLIP(X,L) ( MAX(MIN(X,L), -L) )

static const float tensor_Gemm_0_weight[10][4] = 
{
  {0.34892842173576354980f, 0.95652562379837036133f, -1.2820788621902465820f, -1.0496277809143066406f},
  {0.58661657571792602539f, 0.65637600421905517578f, -0.91158318519592285156f, -1.5556344985961914062f},
  {0.69518995285034179688f, 0.89582788944244384766f, -1.3477854728698730469f, -1.1340354681015014648f},
  {0.013184066861867904663f, -0.045431386679410934448f, 0.019869832322001457214f, -0.61196476221084594727f},
  {-0.13417057693004608154f, -0.41227766871452331543f, 1.4913693666458129883f, 2.1293880939483642578f},
  {0.075728893280029296875f, -0.65949475765228271484f, 0.74042052030563354492f, 1.6367934942245483398f},
  {-0.17601001262664794922f, -0.076551020145416259766f, -0.37412494421005249023f, -0.17246848344802856445f},
  {0.19510932266712188721f, 0.97423881292343139648f, -0.80288153886795043945f, -1.4122152328491210938f},
  {-0.35455858707427978516f, -0.44971632957458496094f, -0.29892426729202270508f, -0.27380108833312988281f},
  {0.15960422158241271973f, 0.063607715070247650146f, 1.2619436979293823242f, 1.0862520933151245117f}
};
static const float tensor_Gemm_0_bias[10] = 
{0.41375762224197387695f, 1.2296770811080932617f, 0.94434416294097900391f, 0.57842272520065307617f, -0.15146249532699584961f, 0.053222063928842544556f, -0.27667647600173950195f, 1.1248825788497924805f, 0.081034302711486816406f, -0.58671587705612182617f};
static const float tensor_Gemm_1_weight[3][10] = 
{
  {1.4417567253112792969f, 0.87651562690734863281f, 2.1815452575683593750f, -0.10679206997156143188f, -2.7294576168060302734f, -1.2938492298126220703f, -0.041788082569837570190f, 1.0375641584396362305f, -0.25967183709144592285f, -2.6738927364349365234f},
  {-0.86683243513107299805f, 0.67778378725051879883f, 0.49807816743850708008f, 0.061375569552183151245f, 0.027746085077524185181f, -0.14653040468692779541f, -0.14176860451698303223f, 0.60825330018997192383f, -0.044471036642789840698f, -0.48771116137504577637f},
  {-0.58054131269454956055f, -2.3596527576446533203f, -2.6783571243286132812f, -0.41116976737976074219f, 1.0583999156951904297f, 0.49656388163566589355f, 0.25472754240036010742f, -2.1370487213134765625f, -0.25759220123291015625f, 0.83260661363601684570f}
};
static const float tensor_Gemm_1_bias[3] = 
{-0.047458216547966003418f, 0.35979986190795898438f, -0.75671666860580444336f};
union tensor_union_0 {
float tensor_onnx__Gemm_5[1][4];
float tensor_onnx__Gemm_7[1][10];
};
static union tensor_union_0 tu0;

union tensor_union_1 {
float tensor_input[1][10];
};
static union tensor_union_1 tu1;


static inline void node_Flatten_0( const float tensor_onnx__Flatten_0[1][4], float tensor_onnx__Gemm_5[1][4] )
{
	/* Flatten*/
	float *input = (float*)tensor_onnx__Flatten_0;
	float *output = (float*)tensor_onnx__Gemm_5;
	for( uint32_t i=0; i<4; i++ )
		output[i] = input[i];

}

static inline void node_Gemm_1( const float tensor_onnx__Gemm_5[1][4], const float tensor_Gemm_0_weight[10][4], const float tensor_Gemm_0_bias[10], float tensor_input[1][10] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 4;
	const int N = 10;
	float (*A)[4]  = (float(*)[4])tensor_onnx__Gemm_5;
	float (*Y)[10]  = (float(*)[10])tensor_input;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[10]  = (float(*)[10])tensor_Gemm_0_bias;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor_Gemm_0_weight[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][c] * beta;
			Y[r][c] = tmp;
	}
}

static inline void node_Relu_2( const float tensor_input[1][10], float tensor_onnx__Gemm_7[1][10] )
{
	/*Relu*/
	float *X = (float*)tensor_input;
	float *Y = (float*)tensor_onnx__Gemm_7;
	for( uint32_t i=0; i<10; i++ )
		Y[i] = X[i] > 0 ? X[i] : 0;

}

static inline void node_Gemm_3( const float tensor_onnx__Gemm_7[1][10], const float tensor_Gemm_1_weight[3][10], const float tensor_Gemm_1_bias[3], float tensor_8[1][3] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 10;
	const int N = 3;
	float (*A)[10]  = (float(*)[10])tensor_onnx__Gemm_7;
	float (*Y)[3]  = (float(*)[3])tensor_8;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[3]  = (float(*)[3])tensor_Gemm_1_bias;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor_Gemm_1_weight[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][c] * beta;
			Y[r][c] = tmp;
	}
}


void entry(const float tensor_onnx__Flatten_0[1][4], float tensor_8[1][3]) {
	node_Flatten_0( tensor_onnx__Flatten_0, tu0.tensor_onnx__Gemm_5);
	node_Gemm_1( tu0.tensor_onnx__Gemm_5, tensor_Gemm_0_weight, tensor_Gemm_0_bias, tu1.tensor_input);
	node_Relu_2( tu1.tensor_input, tu0.tensor_onnx__Gemm_7);
	node_Gemm_3( tu0.tensor_onnx__Gemm_7, tensor_Gemm_1_weight, tensor_Gemm_1_bias, tensor_8);
}
